{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "4. Categorical Variables.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "mount_file_id": "1eduoWwoQAlSIjdMXJg7AZ0KzF-_SN8aX",
      "authorship_tag": "ABX9TyMkNZ5SouPmR3amqARjwbOj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/indrikwijaya/Approaching-Any-ML-Problem/blob/master/4_Categorical_Variables.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# What are categorical variables?\n",
        "- nominal: have 2 or more categories which don't have any kind of order associated with them. For example, if gender is classified into 2 groups, i.e. male & female, it can be considered as a nominal variable\n",
        "- ordinal: have 'levels' or categories with a particular order associated w/ them. For eg, low, medium and high. Order is important\n",
        "\n",
        "- can categorize as **binary**, **cyclic**\n"
      ],
      "metadata": {
        "id": "WcFIpF_xZBMA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd drive/MyDrive/Colab\\ Notebooks/Approaching-Any-ML-Problem"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V8UErcC3axwH",
        "outputId": "6f741571-ce26-4469-913a-a02a9ebca80e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Colab Notebooks/Approaching-Any-ML-Problem\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# cat-in-the-dat Data"
      ],
      "metadata": {
        "id": "isXzNQ_UoYxj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('data/cat_train.csv')\n",
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 325
        },
        "id": "8tgxFOBla5Wo",
        "outputId": "30ee30ee-6679-4826-822f-eb3de91c2a34"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   id  bin_0  bin_1  bin_2 bin_3 bin_4 nom_0      nom_1    nom_2       nom_3  \\\n",
              "0   0    0.0    0.0    0.0     F     N   Red  Trapezoid  Hamster      Russia   \n",
              "1   1    1.0    1.0    0.0     F     Y   Red       Star  Axolotl         NaN   \n",
              "2   2    0.0    1.0    0.0     F     N   Red        NaN  Hamster      Canada   \n",
              "3   3    NaN    0.0    0.0     F     N   Red     Circle  Hamster     Finland   \n",
              "4   4    0.0    NaN    0.0     T     N   Red   Triangle  Hamster  Costa Rica   \n",
              "\n",
              "   ...      nom_9 ord_0        ord_1     ord_2 ord_3 ord_4  ord_5  day month  \\\n",
              "0  ...  02e7c8990   3.0  Contributor       Hot     c     U     Pw  6.0   3.0   \n",
              "1  ...  f37df64af   3.0  Grandmaster      Warm     e     X     pE  7.0   7.0   \n",
              "2  ...        NaN   3.0          NaN  Freezing     n     P     eN  5.0   9.0   \n",
              "3  ...  f9d456e57   1.0       Novice  Lava Hot     a     C    NaN  3.0   3.0   \n",
              "4  ...  c5361037c   3.0  Grandmaster      Cold     h     C     OZ  5.0  12.0   \n",
              "\n",
              "  target  \n",
              "0    0.0  \n",
              "1    0.0  \n",
              "2    0.0  \n",
              "3    0.0  \n",
              "4    0.0  \n",
              "\n",
              "[5 rows x 25 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-28af0481-a860-4dd4-ab46-1d215886b7d3\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>bin_0</th>\n",
              "      <th>bin_1</th>\n",
              "      <th>bin_2</th>\n",
              "      <th>bin_3</th>\n",
              "      <th>bin_4</th>\n",
              "      <th>nom_0</th>\n",
              "      <th>nom_1</th>\n",
              "      <th>nom_2</th>\n",
              "      <th>nom_3</th>\n",
              "      <th>...</th>\n",
              "      <th>nom_9</th>\n",
              "      <th>ord_0</th>\n",
              "      <th>ord_1</th>\n",
              "      <th>ord_2</th>\n",
              "      <th>ord_3</th>\n",
              "      <th>ord_4</th>\n",
              "      <th>ord_5</th>\n",
              "      <th>day</th>\n",
              "      <th>month</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>F</td>\n",
              "      <td>N</td>\n",
              "      <td>Red</td>\n",
              "      <td>Trapezoid</td>\n",
              "      <td>Hamster</td>\n",
              "      <td>Russia</td>\n",
              "      <td>...</td>\n",
              "      <td>02e7c8990</td>\n",
              "      <td>3.0</td>\n",
              "      <td>Contributor</td>\n",
              "      <td>Hot</td>\n",
              "      <td>c</td>\n",
              "      <td>U</td>\n",
              "      <td>Pw</td>\n",
              "      <td>6.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>F</td>\n",
              "      <td>Y</td>\n",
              "      <td>Red</td>\n",
              "      <td>Star</td>\n",
              "      <td>Axolotl</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>f37df64af</td>\n",
              "      <td>3.0</td>\n",
              "      <td>Grandmaster</td>\n",
              "      <td>Warm</td>\n",
              "      <td>e</td>\n",
              "      <td>X</td>\n",
              "      <td>pE</td>\n",
              "      <td>7.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>F</td>\n",
              "      <td>N</td>\n",
              "      <td>Red</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Hamster</td>\n",
              "      <td>Canada</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>3.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Freezing</td>\n",
              "      <td>n</td>\n",
              "      <td>P</td>\n",
              "      <td>eN</td>\n",
              "      <td>5.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>F</td>\n",
              "      <td>N</td>\n",
              "      <td>Red</td>\n",
              "      <td>Circle</td>\n",
              "      <td>Hamster</td>\n",
              "      <td>Finland</td>\n",
              "      <td>...</td>\n",
              "      <td>f9d456e57</td>\n",
              "      <td>1.0</td>\n",
              "      <td>Novice</td>\n",
              "      <td>Lava Hot</td>\n",
              "      <td>a</td>\n",
              "      <td>C</td>\n",
              "      <td>NaN</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>T</td>\n",
              "      <td>N</td>\n",
              "      <td>Red</td>\n",
              "      <td>Triangle</td>\n",
              "      <td>Hamster</td>\n",
              "      <td>Costa Rica</td>\n",
              "      <td>...</td>\n",
              "      <td>c5361037c</td>\n",
              "      <td>3.0</td>\n",
              "      <td>Grandmaster</td>\n",
              "      <td>Cold</td>\n",
              "      <td>h</td>\n",
              "      <td>C</td>\n",
              "      <td>OZ</td>\n",
              "      <td>5.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 25 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-28af0481-a860-4dd4-ab46-1d215886b7d3')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-28af0481-a860-4dd4-ab46-1d215886b7d3 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-28af0481-a860-4dd4-ab46-1d215886b7d3');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset consists of all kinds of categorical variables:\n",
        "\n",
        "• Nominal\n",
        "\n",
        "• Ordinal\n",
        "\n",
        "• Cyclical\n",
        "\n",
        "• Binary\n",
        "\n",
        "Overall, there are:\n",
        "\n",
        "• Five binary variables\n",
        "\n",
        "• Ten nominal variables\n",
        "\n",
        "• Six ordinal variables\n",
        "\n",
        "• Two cyclic variables\n",
        "\n",
        "• And a target variable"
      ],
      "metadata": {
        "id": "yFxm3Hwla-h2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.target.value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1YFcwv9Ucmxj",
        "outputId": "9d0810af-f4a1-4200-b4c8-6a183a08434f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0    78347\n",
              "1.0    18166\n",
              "Name: target, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Classes are skewed -> best metric for this binary classification problem is AUC"
      ],
      "metadata": {
        "id": "fLVV-ha0c5k_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.ord_2.unique()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hUvDZ_AiczRV",
        "outputId": "75580a69-a8a2-46d6-920d-4fd729bcc290"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['Hot', 'Warm', 'Freezing', 'Lava Hot', 'Cold', 'Boiling Hot', nan],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mapping = {\"Freezing\": 0,\n",
        "  \"Warm\": 1,\n",
        "  \"Cold\": 2,\n",
        "  \"Boiling Hot\": 3,\n",
        "  \"Hot\": 4,\n",
        "  \"Lava Hot\": 5\n",
        "}"
      ],
      "metadata": {
        "id": "tGuXnrPHdAf7"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.loc[:, \"ord_2\"] = df.ord_2.map(mapping)\n",
        "df.ord_2.value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3KRbi46ZdF-g",
        "outputId": "45355cef-0e0d-47db-e196-3fd96ca4f9f2"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0    23009\n",
              "1.0    20028\n",
              "2.0    15557\n",
              "3.0    13802\n",
              "4.0    10879\n",
              "5.0    10395\n",
              "Name: ord_2, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can use `LabelEncoder` instead of doing the mapping manually"
      ],
      "metadata": {
        "id": "3nlWkkDZdKvb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd src\n",
        "from sklearn import preprocessing\n",
        "# read the data\n",
        "df = pd.read_csv(\"../data/cat_train.csv\")\n",
        "# fill NaN values in ord_2 column\n",
        "df.loc[:, \"ord_2\"] = df.ord_2.fillna(\"NONE\")\n",
        "# initialize LabelEncoder\n",
        "lbl_enc = preprocessing.LabelEncoder()\n",
        "# fit label encoder and transform values on ord_2 column\n",
        "# P.S: do not use this directly. fit first, then transform\n",
        "df.loc[:, \"ord_2\"] = lbl_enc.fit_transform(df.ord_2.values)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YMRF5K8tdSAb",
        "outputId": "46de0c9c-e561-4f24-f975-345cacacde2f"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Errno 2] No such file or directory: 'src'\n",
            "/content/drive/MyDrive/Colab Notebooks/Approaching-Any-ML-Problem/src\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "It becomes easy to store lots of binarized variables like this if we store them in a\n",
        "sparse format. A sparse format is nothing but a representation or way of storing\n",
        "data in memory in which you do not store all the values but only the values that\n",
        "matter. In the case of binary variables described above, all that matters is where we\n",
        "have ones (1s).\n",
        "\n",
        "Even though the sparse representation of binarized features takes much less\n",
        "memory than its dense representation, there is another transformation for\n",
        "categorical variables that takes even less memory. This is known as One Hot\n",
        "Encoding.\n",
        "One hot encoding is a binary encoding too in the sense that there are only two\n",
        "values, 0s and 1s. However, it must be noted that it’s not a binary representation.\n",
        "\n",
        "We can also convert them into numerical variables.\n",
        "\n",
        "Suppose we go back to the categorical features dataframe (original cat-in-the-datii)\n",
        "that we had. How many ids do we have in the dataframe where the value of ord_2\n",
        "is Boiling Hot ?\n",
        "We can easily calculate this value by calculating the shape of the dataframe where\n",
        "ord_2 column has the value Boiling Hot.\n"
      ],
      "metadata": {
        "id": "jeHOBg1wdXRH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Boiling hot = 3\n",
        "df[df.ord_2 == 3].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eVya8HsKdokT",
        "outputId": "96d78c8b-913b-4436-ea31-aa903b089266"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10879, 25)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.groupby([\"ord_2\"])[\"id\"].count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oyUlYA38eM4q",
        "outputId": "35c079e2-a69f-45e8-e6ad-b09d70ae2a21"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ord_2\n",
              "0    13802\n",
              "1    15557\n",
              "2    23009\n",
              "3    10879\n",
              "4    10395\n",
              "5     2844\n",
              "6    20028\n",
              "Name: id, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If we just replace ord_2 column with its count values, we have converted it to a\n",
        "feature which is kind of numerical now. We can create a new column or replace this\n",
        "column by using the `transform` function of pandas along with `groupby`."
      ],
      "metadata": {
        "id": "PMqDODNleWbc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.groupby(['ord_2'])['id'].transform('count')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9p3WN_VdebYz",
        "outputId": "263fb3b6-871f-4bb6-db63-869c5a7a48b5"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0        10879\n",
              "1        20028\n",
              "2        23009\n",
              "3        10395\n",
              "4        15557\n",
              "         ...  \n",
              "96509    15557\n",
              "96510    20028\n",
              "96511    15557\n",
              "96512    23009\n",
              "96513    23009\n",
              "Name: id, Length: 96514, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can add counts of all the features or can also replace them or maybe group by\n",
        "multiple columns and their counts. For example, the following code counts by\n",
        "grouping on ord_1 and ord_2 columns."
      ],
      "metadata": {
        "id": "8GEN9H0hehmr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.groupby(\n",
        "    [\n",
        "    \"ord_1\",\n",
        "    \"ord_2\"]\n",
        "    )[\"id\"].count().reset_index(name=\"count\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "os6QtNuRep1G",
        "outputId": "50a29347-03da-4a23-eeda-a1411c63cf60"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "          ord_1  ord_2  count\n",
              "0   Contributor      0   2531\n",
              "1   Contributor      1   2832\n",
              "2   Contributor      2   4235\n",
              "3   Contributor      3   2016\n",
              "4   Contributor      4   1934\n",
              "5   Contributor      5    523\n",
              "6   Contributor      6   3666\n",
              "7        Expert      0   3243\n",
              "8        Expert      1   3596\n",
              "9        Expert      2   5327\n",
              "10       Expert      3   2521\n",
              "11       Expert      4   2368\n",
              "12       Expert      5    677\n",
              "13       Expert      6   4672\n",
              "14  Grandmaster      0   2180\n",
              "15  Grandmaster      1   2476\n",
              "16  Grandmaster      2   3682\n",
              "17  Grandmaster      3   1733\n",
              "18  Grandmaster      4   1646\n",
              "19  Grandmaster      5    451\n",
              "20  Grandmaster      6   3221\n",
              "21       Master      0   1737\n",
              "22       Master      1   2030\n",
              "23       Master      2   2927\n",
              "24       Master      3   1357\n",
              "25       Master      4   1329\n",
              "26       Master      5    349\n",
              "27       Master      6   2583\n",
              "28       Novice      0   3672\n",
              "29       Novice      1   4144\n",
              "30       Novice      2   6096\n",
              "31       Novice      3   2937\n",
              "32       Novice      4   2807\n",
              "33       Novice      5    760\n",
              "34       Novice      6   5297"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-37c1cea3-670e-4506-8de5-6fb85de4d99d\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ord_1</th>\n",
              "      <th>ord_2</th>\n",
              "      <th>count</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Contributor</td>\n",
              "      <td>0</td>\n",
              "      <td>2531</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Contributor</td>\n",
              "      <td>1</td>\n",
              "      <td>2832</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Contributor</td>\n",
              "      <td>2</td>\n",
              "      <td>4235</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Contributor</td>\n",
              "      <td>3</td>\n",
              "      <td>2016</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Contributor</td>\n",
              "      <td>4</td>\n",
              "      <td>1934</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Contributor</td>\n",
              "      <td>5</td>\n",
              "      <td>523</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Contributor</td>\n",
              "      <td>6</td>\n",
              "      <td>3666</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Expert</td>\n",
              "      <td>0</td>\n",
              "      <td>3243</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Expert</td>\n",
              "      <td>1</td>\n",
              "      <td>3596</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Expert</td>\n",
              "      <td>2</td>\n",
              "      <td>5327</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>Expert</td>\n",
              "      <td>3</td>\n",
              "      <td>2521</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>Expert</td>\n",
              "      <td>4</td>\n",
              "      <td>2368</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>Expert</td>\n",
              "      <td>5</td>\n",
              "      <td>677</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>Expert</td>\n",
              "      <td>6</td>\n",
              "      <td>4672</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>Grandmaster</td>\n",
              "      <td>0</td>\n",
              "      <td>2180</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>Grandmaster</td>\n",
              "      <td>1</td>\n",
              "      <td>2476</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>Grandmaster</td>\n",
              "      <td>2</td>\n",
              "      <td>3682</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>Grandmaster</td>\n",
              "      <td>3</td>\n",
              "      <td>1733</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>Grandmaster</td>\n",
              "      <td>4</td>\n",
              "      <td>1646</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>Grandmaster</td>\n",
              "      <td>5</td>\n",
              "      <td>451</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>Grandmaster</td>\n",
              "      <td>6</td>\n",
              "      <td>3221</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>Master</td>\n",
              "      <td>0</td>\n",
              "      <td>1737</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>Master</td>\n",
              "      <td>1</td>\n",
              "      <td>2030</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>Master</td>\n",
              "      <td>2</td>\n",
              "      <td>2927</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>Master</td>\n",
              "      <td>3</td>\n",
              "      <td>1357</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>Master</td>\n",
              "      <td>4</td>\n",
              "      <td>1329</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>Master</td>\n",
              "      <td>5</td>\n",
              "      <td>349</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>Master</td>\n",
              "      <td>6</td>\n",
              "      <td>2583</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>Novice</td>\n",
              "      <td>0</td>\n",
              "      <td>3672</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>Novice</td>\n",
              "      <td>1</td>\n",
              "      <td>4144</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>Novice</td>\n",
              "      <td>2</td>\n",
              "      <td>6096</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>Novice</td>\n",
              "      <td>3</td>\n",
              "      <td>2937</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>Novice</td>\n",
              "      <td>4</td>\n",
              "      <td>2807</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>Novice</td>\n",
              "      <td>5</td>\n",
              "      <td>760</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>Novice</td>\n",
              "      <td>6</td>\n",
              "      <td>5297</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-37c1cea3-670e-4506-8de5-6fb85de4d99d')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-37c1cea3-670e-4506-8de5-6fb85de4d99d button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-37c1cea3-670e-4506-8de5-6fb85de4d99d');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "One more trick is to create new features from these categorical variables. You can\n",
        "create new categorical features from existing features, and this can be done in an\n",
        "effortless manner."
      ],
      "metadata": {
        "id": "mYY1oagAeymP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"new_feature\"] = (\n",
        "  df.ord_1.astype(str)\n",
        "  + \"_\"\n",
        "  + df.ord_2.astype(str)\n",
        "  )"
      ],
      "metadata": {
        "id": "00tcqIj_e4l7"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, we have combined ord_1 and ord_2 by an underscore, and before that, we\n",
        "convert these columns to string types. Note that NaN will also convert to string. But\n",
        "it’s okay. We can also treat NaN as a new category. Thus, we have a new feature\n",
        "which is a combination of these two features. You can also combine more than three\n",
        "columns or four or even more.\n",
        "\n",
        "Whenever you get categorical variables, follow these simple steps:\n",
        "- fill the NaN values (this is very important!)\n",
        "- convert them to integers by applying label encoding using LabelEncoder\n",
        "of scikit-learn or by using a mapping dictionary. If you didn’t fill up NaN\n",
        "values with something, you might have to take care of them in this step\n",
        "- create one-hot encoding\n",
        "- Go for modeling\n",
        "\n",
        "Another way of\n",
        "handling NaN values is to treat them as a completely new category. This is the most\n",
        "preferred way of handling NaN values. And can be achieved in a very simple\n",
        "manner if you are using pandas."
      ],
      "metadata": {
        "id": "33j50RfPe8Nz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.ord_2.fillna(\"NONE\").value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8o-craJue_KB",
        "outputId": "fc180331-da24-4b1a-934f-f52130f41221"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2    23009\n",
              "6    20028\n",
              "1    15557\n",
              "0    13802\n",
              "3    10879\n",
              "4    10395\n",
              "5     2844\n",
              "Name: ord_2, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Wow! There were 20028 NaN values in this column that we didn’t even consider\n",
        "using previously. With the addition of this new category, the total number of\n",
        "categories have now increased from 6 to 7. This is okay because now when we build\n",
        "our models, we will also consider NaN. The more relevant information we have,\n",
        "the better the model is.\n",
        "\n",
        "Let’s assume that ord_2 did not have any NaN values. We see that all categories in\n",
        "this column have a significant count. There are no “rare” categories; i.e. the\n",
        "categories which appear only a small percentage of the total number of samples.\n",
        "Now, let’s assume that you have deployed this model which uses this column in\n",
        "production and when the model or the project is live, you get a category in ord_2\n",
        "column that is not present in train. You model pipeline, in this case, will throw an\n",
        "error and there is nothing that you can do about it. If this happens, then probably\n",
        "something is wrong with your pipeline in production. If this is expected, then you\n",
        "must modify your model pipeline and include a new category to these six categories.\n",
        "\n",
        "This new category is known as the “rare” category. A rare category is a category\n",
        "which is not seen very often and can include many different categories. You can\n",
        "also try to “predict” the unknown category by using a nearest neighbour model.\n",
        "Remember, if you predict this category, it will become one of the categories from\n",
        "the training data.\n",
        "\n",
        "If you have a fixed test set, you can add your test data to training to know about the\n",
        "categories in a given feature. This is very similar to semi-supervised learning in\n",
        "which you use data which is not available for training to improve your model. This\n",
        "will also take care of rare values that appear very less number of times in training\n",
        "data but are in abundance in test data. Your model will be more robust.\n",
        "Many people think that this idea overfits. It may or may not overfit. There is a\n",
        "simple fix for that. If you design your cross-validation in such a way that it\n",
        "replicates the prediction process when you run your model on test data, then it’s\n",
        "never going to overfit. It means that the first step should be the separation of folds,\n",
        "and in each fold, you should apply the same pre-processing that you want to apply\n",
        "to test data. Suppose you want to concatenate training and test data, then in each\n",
        "fold you must concatenate training and validation data and also make sure that your\n",
        "validation dataset replicates the test set. In this specific case, you must design your\n",
        "validation sets in such a way that it has categories which are “unseen” in the training\n",
        "set."
      ],
      "metadata": {
        "id": "618nmpZFfV3K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# read training data\n",
        "train = pd.read_csv(\"../data/cat_train.csv\")\n",
        "#read test data\n",
        "test = pd.read_csv(\"../data/cat_test.csv\")\n",
        "\n",
        "# create a fake target column for test data\n",
        "# since this column doesn't exist\n",
        "test.loc[:, \"target\"] = -1\n",
        "\n",
        "# concatenate both training and test data\n",
        "data = pd.concat([train, test]).reset_index(drop=True)\n",
        "\n",
        "# make a list of features we are interested in\n",
        "# id and target is something we should not encode\n",
        "features = [x for x in train.columns if x not in [\"id\", \"target\"]]\n",
        "\n",
        "# loop over the features list\n",
        "for feat in features:\n",
        "  # create a new instance of LabelEncoder for each feature\n",
        "  lbl_enc = preprocessing.LabelEncoder()\n",
        "  # note the trick here\n",
        "  # since its categorical data, we fillna with a string\n",
        "  # and we convert all the data to string type\n",
        "  # so, no matter its int or float, its converted to string\n",
        "  # int/float but categorical!!!\n",
        "  temp_col = data[feat].fillna(\"NONE\").astype(str).values\n",
        "  \n",
        "  # we can use fit_transform here as we do not\n",
        "  # have any extra test data that we need to\n",
        "  # transform on separately\n",
        "  data.loc[:, feat] = lbl_enc.fit_transform(temp_col)\n",
        "\n",
        "# split the training and test data again\n",
        "train = data[data.target != -1].reset_index(drop=True)\n",
        "test = data[data.target == -1].reset_index(drop=True)"
      ],
      "metadata": {
        "id": "qoF2RVxXfibM"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This trick works when you have a problem where you already have the test dataset.\n",
        "It must be noted that this trick will not work in a live setting. For example, let’s say\n",
        "you are in a company that builds a real-time bidding solution (RTB). RTB systems\n",
        "bid on every user they see online to buy ad space. The features that can be used for\n",
        "such a model may include pages viewed in a website. Let’s assume that features are\n",
        "the last five categories/pages visited by the user. In this case, if the website\n",
        "introduces new categories, we will no longer be able to predict accurately. Our\n",
        "model, in this case, will fail. A situation like this can be avoided by using an\n",
        "“unknown” category.\n",
        "\n",
        "We can treat “NONE” as unknown. So, if during live testing, we get new categories\n",
        "that we have not seen before, we will mark them as “NONE”.\n",
        "This is very similar to natural language processing problems. We always build a\n",
        "model based on a fixed vocabulary. Increasing the size of the vocabulary increases\n",
        "the size of the model. Transformer models like BERT are trained on ~30000 words\n",
        "(for English). So, when we have a new word coming in, we mark it as UNK\n",
        "(unknown).\n",
        "So, you can either assume that your test data will have the same categories as\n",
        "training or you can introduce a rare or unknown category to training to take care of\n",
        "new categories in test data.\n",
        "Let’s see the value counts in ord_4 column after filling NaN values"
      ],
      "metadata": {
        "id": "eZ4oTbLIgO5S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.ord_4.fillna(\"NONE\").value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tSgXoGbGgz0P",
        "outputId": "a01d27db-dd71-4395-dab0-7ae624c20380"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "N       6395\n",
              "P       6014\n",
              "Y       5904\n",
              "A       5803\n",
              "C       5396\n",
              "M       5314\n",
              "X       5253\n",
              "U       5226\n",
              "R       5226\n",
              "H       5036\n",
              "T       4942\n",
              "Q       4865\n",
              "O       4171\n",
              "B       4114\n",
              "E       3450\n",
              "K       3398\n",
              "I       3254\n",
              "NONE    2810\n",
              "D       2791\n",
              "F       2565\n",
              "W       1384\n",
              "Z        912\n",
              "S        705\n",
              "G        513\n",
              "V        488\n",
              "J        303\n",
              "L        282\n",
              "Name: ord_4, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can now define our criteria for calling a value “rare”. Let’s say the requirement\n",
        "for a value being rare in this column is a count of less than 1000. So, it seems, Z, S, G, V, J and\n",
        "L can be marked as rare values. With pandas, it is quite easy to replace categories\n",
        "based on count threshold. Let’s take a look at how it’s done."
      ],
      "metadata": {
        "id": "jmhRSu1ng5ER"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.ord_4 = df.ord_4.fillna(\"NONE\")"
      ],
      "metadata": {
        "id": "yEkXO3kZhMdf"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.loc[\n",
        "  df[\"ord_4\"].value_counts()[df[\"ord_4\"]].values < 2000,\n",
        "  \"ord_4\"\n",
        "  ] = \"RARE\""
      ],
      "metadata": {
        "id": "XTVG2BcbhQKk"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.ord_4.value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5iM5hqjihU1n",
        "outputId": "ed6b9742-271e-461e-9aba-2deaa92b84be"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "N       6395\n",
              "P       6014\n",
              "Y       5904\n",
              "A       5803\n",
              "C       5396\n",
              "M       5314\n",
              "X       5253\n",
              "U       5226\n",
              "R       5226\n",
              "H       5036\n",
              "T       4942\n",
              "Q       4865\n",
              "RARE    4587\n",
              "O       4171\n",
              "B       4114\n",
              "E       3450\n",
              "K       3398\n",
              "I       3254\n",
              "NONE    2810\n",
              "D       2791\n",
              "F       2565\n",
              "Name: ord_4, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We say that wherever the value count for a certain category is less than 2000,\n",
        "replace it with rare. So, now, when it comes to test data, all the new, unseen\n",
        "categories will be mapped to “RARE”, and all missing values will be mapped to\n",
        "“NONE”.\n",
        "This approach will also ensure that the model works in a live setting, even if you\n",
        "have new categories.\n",
        "Now we have everything we need to approach any kind of problem with categorical\n",
        "variables in it. Let’s try building our first model and try to improve its performance\n",
        "in a step-wise manner.\n",
        "Before going to any kind of model building, it’s essential to take care of crossvalidation.\n",
        "We have already seen the label/target distribution, and we know that it\n",
        "is a binary classification problem with skewed targets. Thus, we will be using\n",
        "StratifiedKFold to split the data here."
      ],
      "metadata": {
        "id": "68_q7zhHhWDh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile create_folds.py\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn import model_selection\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  # Read training data\n",
        "  df = pd.read_csv(\"../data/cat_train.csv\")\n",
        "  \n",
        "  # we create a new column called kfold and fill it with -1\n",
        "  df[\"kfold\"] = -1\n",
        "  \n",
        "  # the next step is to randomize the rows of the data\n",
        "  df = df.sample(frac=1).reset_index(drop=True)\n",
        "  \n",
        "  # fillna with 0\n",
        "  df = df.fillna(0)\n",
        "\n",
        "  # fetch labels\n",
        "  y = df.target.values\n",
        "  \n",
        "  # initiate the kfold class from model_selection module\n",
        "  kf = model_selection.StratifiedKFold(n_splits=5)\n",
        "  \n",
        "  # fill the new kfold column\n",
        "  for f, (t_, v_) in enumerate(kf.split(X=df, y=y)):\n",
        "    df.loc[v_, 'kfold'] = f\n",
        "\n",
        "  # save the new csv with kfold column  \n",
        "  df.to_csv(\"../input/cat_train_folds.csv\", index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ipqcjaBMheui",
        "outputId": "56a8fa1b-4053-443f-bc84-c990f63b7ca4"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting create_folds.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python create_folds.py"
      ],
      "metadata": {
        "id": "2jwqZKfHiGxE"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('../input/cat_train_folds.csv')\n",
        "df.kfold.value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8QxOSgggiJdY",
        "outputId": "87d9fc42-4557-422c-d097-904dfb1104df"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    19303\n",
              "1    19303\n",
              "2    19303\n",
              "3    19303\n",
              "4    19302\n",
              "Name: kfold, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(5):\n",
        "  print(i, df[df.kfold==i].target.value_counts())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nQe7-peHi2tb",
        "outputId": "8cb0c97e-cc5e-4110-b4e5-c3661a3c5c06"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 0.0    15669\n",
            "1.0     3634\n",
            "Name: target, dtype: int64\n",
            "1 0.0    15670\n",
            "1.0     3633\n",
            "Name: target, dtype: int64\n",
            "2 0.0    15670\n",
            "1.0     3633\n",
            "Name: target, dtype: int64\n",
            "3 0.0    15670\n",
            "1.0     3633\n",
            "Name: target, dtype: int64\n",
            "4 0.0    15669\n",
            "1.0     3633\n",
            "Name: target, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Logistic Regression w/ OneHot encoding"
      ],
      "metadata": {
        "id": "JZDEJdGhmtZK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Build a simple model using logistic regression"
      ],
      "metadata": {
        "id": "j2QWeu5MjMDt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile ohe_logres.py\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "def run(fold):\n",
        "  # Load the full training data with folds\n",
        "  df = pd.read_csv('../input/cat_train_folds.csv')\n",
        "\n",
        "  # all columns are features except id, target and kfold columns\n",
        "  features = [\n",
        "  f for f in df.columns if f not in (\"id\", \"target\", \"kfold\")\n",
        "  ]\n",
        "  # fill all NaN values with NONE\n",
        "  # note that I am converting all columns to \"strings\"\n",
        "  # it doesn’t matter because all are categories\n",
        "\n",
        "  for col in features:\n",
        "    df.loc[:, col] = df[col].astype(str).fillna(\"NONE\")\n",
        "\n",
        "  # get training data using folds\n",
        "  df_train = df[df.kfold != fold].reset_index(drop=True)\n",
        "\n",
        "  # get validation data using folds\n",
        "  df_valid = df[df.kfold == fold].reset_index(drop=True)\n",
        "\n",
        "  # initialize OneHotEncoder from scikit-learn\n",
        "  ohe = OneHotEncoder()\n",
        "  # fit ohe on training + validation features\n",
        "  full_data = pd.concat(\n",
        "  [df_train[features], df_valid[features]],\n",
        "  axis=0\n",
        "  )\n",
        "\n",
        "  ohe.fit(full_data[features])\n",
        "  \n",
        "  # transform training data\n",
        "  x_train = ohe.transform(df_train[features])\n",
        "  \n",
        "  # transform validation data\n",
        "  x_valid = ohe.transform(df_valid[features])\n",
        "  \n",
        "  # initialize Logistic Regression model\n",
        "  model = LogisticRegression()\n",
        "\n",
        "  # fit model on training data (ohe)\n",
        "  model.fit(x_train, df_train.target.values)\n",
        "\n",
        "  # predict on validation data\n",
        "  # we need the probability values as we are calculating AUC\n",
        "  # we will use the probability of 1s\n",
        "  valid_preds = model.predict_proba(x_valid)[:, 1]\n",
        "\n",
        "  # get roc auc score\n",
        "  auc = roc_auc_score(df_valid.target.values, valid_preds)\n",
        "\n",
        "  # print auc\n",
        "  print(auc)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  run(0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PIHN2KiMjXPy",
        "outputId": "8bd7cf1c-fe57-4594-a62b-25b8113cbd5e"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting ohe_logres.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python ohe_logres.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hWEmfVWfknrU",
        "outputId": "20601685-7c8f-413b-d00f-a93c1d021120"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "0.7847864978358783\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are a few warnings. It seems logistic regression did not converge for the max\n",
        "number of iterations. We didn’t play with the parameters, so that is fine. We see\n",
        "that AUC is ~ 0.785.\n",
        "Let’s run it for all folds now with a simple change in code."
      ],
      "metadata": {
        "id": "s2X7RHQrkpYi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile ohe_logres.py\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "def run(fold):\n",
        "  # Load the full training data with folds\n",
        "  df = pd.read_csv('../input/cat_train_folds.csv')\n",
        "\n",
        "  # all columns are features except id, target and kfold columns\n",
        "  features = [\n",
        "  f for f in df.columns if f not in (\"id\", \"target\", \"kfold\")\n",
        "  ]\n",
        "  # fill all NaN values with NONE\n",
        "  # note that I am converting all columns to \"strings\"\n",
        "  # it doesn’t matter because all are categories\n",
        "\n",
        "  for col in features:\n",
        "    df.loc[:, col] = df[col].astype(str).fillna(\"NONE\")\n",
        "\n",
        "  # get training data using folds\n",
        "  df_train = df[df.kfold != fold].reset_index(drop=True)\n",
        "\n",
        "  # get validation data using folds\n",
        "  df_valid = df[df.kfold == fold].reset_index(drop=True)\n",
        "\n",
        "  # initialize OneHotEncoder from scikit-learn\n",
        "  ohe = OneHotEncoder()\n",
        "  # fit ohe on training + validation features\n",
        "  full_data = pd.concat(\n",
        "  [df_train[features], df_valid[features]],\n",
        "  axis=0\n",
        "  )\n",
        "\n",
        "  ohe.fit(full_data[features])\n",
        "  \n",
        "  # transform training data\n",
        "  x_train = ohe.transform(df_train[features])\n",
        "  \n",
        "  # transform validation data\n",
        "  x_valid = ohe.transform(df_valid[features])\n",
        "  \n",
        "  # initialize Logistic Regression model\n",
        "  model = LogisticRegression()\n",
        "\n",
        "  # fit model on training data (ohe)\n",
        "  model.fit(x_train, df_train.target.values)\n",
        "\n",
        "  # predict on validation data\n",
        "  # we need the probability values as we are calculating AUC\n",
        "  # we will use the probability of 1s\n",
        "  valid_preds = model.predict_proba(x_valid)[:, 1]\n",
        "\n",
        "  # get roc auc score\n",
        "  auc = roc_auc_score(df_valid.target.values, valid_preds)\n",
        "\n",
        "  # print auc\n",
        "  print(f\"Fold = {fold}, AUC = {auc}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  for fold_ in range(5):\n",
        "    run(fold_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IhT1heEBlDNn",
        "outputId": "d1e60dc1-06ea-4712-ff9d-31b6d0da6ea8"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting ohe_logres.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -W ignore ohe_logres.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6cEzlZGjlMTq",
        "outputId": "30cd00d5-fc21-4d05-bf1f-5a6f4b0e437e"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold = 0, AUC = 0.7847864978358783\n",
            "Fold = 1, AUC = 0.7853553678923606\n",
            "Fold = 2, AUC = 0.7879321947478755\n",
            "Fold = 3, AUC = 0.7870315760687687\n",
            "Fold = 4, AUC = 0.7864695667409296\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We see that AUC scores are quite stable across all folds. The average AUC is\n",
        "0.78631449527. Quite good for our first model!\n",
        "Many people will start this kind of problem with a tree-based model, such as\n",
        "random forest. For applying random forest in this dataset, instead of one-hot\n",
        "encoding, we can use label encoding and convert every feature in every column to\n",
        "an integer as discussed previously."
      ],
      "metadata": {
        "id": "iabiGHeTlN7P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Random Forest w/ LabelEncoder"
      ],
      "metadata": {
        "id": "xhlOeTpQmxFR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile lbl_rf.py\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "def run(fold):\n",
        "  # Load the full training data with folds\n",
        "  df = pd.read_csv('../input/cat_train_folds.csv')\n",
        "\n",
        "  # all columns are features except id, target and kfold columns\n",
        "  features = [\n",
        "  f for f in df.columns if f not in (\"id\", \"target\", \"kfold\")\n",
        "  ]\n",
        "  \n",
        "  # fill all NaN values with NONE\n",
        "  # note that I am converting all columns to \"strings\"\n",
        "  # it doesn’t matter because all are categories\n",
        "  for col in features:\n",
        "    df.loc[:, col] = df[col].astype(str).fillna(\"NONE\")\n",
        "\n",
        "  # label encode the features\n",
        "  for col in features:\n",
        "    lbl = LabelEncoder()\n",
        "\n",
        "    # fit label encoder on all data\n",
        "    lbl.fit(df[col])\n",
        "\n",
        "    # transform all the data\n",
        "    df.loc[:, col] = lbl.transform(df[col])\n",
        "\n",
        "  # get training data using folds\n",
        "  df_train = df[df.kfold != fold].reset_index(drop=True)\n",
        "\n",
        "  # get validation data using folds\n",
        "  df_valid = df[df.kfold == fold].reset_index(drop=True)\n",
        "  \n",
        "  # get training data\n",
        "  x_train = df_train[features].values\n",
        "  \n",
        "  # get validation data\n",
        "  x_valid = df_valid[features].values\n",
        "  \n",
        "  # initialize Random Forest model\n",
        "  model = RandomForestClassifier(n_jobs=-1)\n",
        "\n",
        "  # fit model on training data (ohe)\n",
        "  model.fit(x_train, df_train.target.values)\n",
        "\n",
        "  # predict on validation data\n",
        "  # we need the probability values as we are calculating AUC\n",
        "  # we will use the probability of 1s\n",
        "  valid_preds = model.predict_proba(x_valid)[:, 1]\n",
        "\n",
        "  # get roc auc score\n",
        "  auc = roc_auc_score(df_valid.target.values, valid_preds)\n",
        "\n",
        "  # print auc\n",
        "  print(f\"Fold = {fold}, AUC = {auc}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  for fold_ in range(5):\n",
        "    run(fold_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7FT3wwlLldEb",
        "outputId": "1ea64870-dfcc-4646-ef3d-7c81418aee7e"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting lbl_rf.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python lbl_rf.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RxRfT4ajmRGf",
        "outputId": "eaa1e3f2-1aba-4fbf-b953-dde1955d6b78"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold = 0, AUC = 0.716930900090115\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\", line 935, in retrieve\n",
            "    self._output.extend(job.get(timeout=self.timeout))\n",
            "  File \"/usr/lib/python3.7/multiprocessing/pool.py\", line 651, in get\n",
            "    self.wait(timeout)\n",
            "  File \"/usr/lib/python3.7/multiprocessing/pool.py\", line 648, in wait\n",
            "    self._event.wait(timeout)\n",
            "  File \"/usr/lib/python3.7/threading.py\", line 552, in wait\n",
            "    signaled = self._cond.wait(timeout)\n",
            "  File \"/usr/lib/python3.7/threading.py\", line 296, in wait\n",
            "    waiter.acquire()\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"lbl_rf.py\", line 64, in <module>\n",
            "    run(fold_)\n",
            "  File \"lbl_rf.py\", line 49, in run\n",
            "    model.fit(x_train, df_train.target.values)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/_forest.py\", line 467, in fit\n",
            "    for i, t in enumerate(trees)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\", line 1056, in __call__\n",
            "    self.retrieve()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\", line 957, in retrieve\n",
            "    backend.abort_everything(ensure_ready=ensure_ready)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/joblib/_parallel_backends.py\", line 257, in abort_everything\n",
            "    self.terminate()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/joblib/_parallel_backends.py\", line 243, in terminate\n",
            "    self._pool.terminate()  # terminate does a join()\n",
            "  File \"/usr/lib/python3.7/multiprocessing/pool.py\", line 548, in terminate\n",
            "    self._terminate()\n",
            "  File \"/usr/lib/python3.7/multiprocessing/util.py\", line 224, in __call__\n",
            "    res = self._callback(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.7/multiprocessing/pool.py\", line 594, in _terminate_pool\n",
            "    worker_handler.join()\n",
            "  File \"/usr/lib/python3.7/threading.py\", line 1044, in join\n",
            "    self._wait_for_tstate_lock()\n",
            "  File \"/usr/lib/python3.7/threading.py\", line 1060, in _wait_for_tstate_lock\n",
            "    elif lock.acquire(block, timeout):\n",
            "KeyboardInterrupt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Wow! Huge difference! The random forest model, without any tuning of\n",
        "hyperparameters, performs a lot worse than simple logistic regression.\n",
        "\n",
        "And this is a reason why we should always start with simple models first. A fan of\n",
        "random forest would begin with it here and will ignore logistic regression model\n",
        "thinking it’s a very simple model that cannot bring any value better than random\n",
        "forest. That kind of person will make a huge mistake. In our implementation of\n",
        "random forest, the folds take a much longer time to complete compared to logistic\n",
        "regression. So, we are not only losing on AUC but also taking much longer to\n",
        "complete the training. Please note that inference is also time-consuming with\n",
        "random forest and it also takes much larger space.\n",
        "\n",
        "If we want, we can also try to run random forest on sparse one-hot encoded data,\n",
        "but that is going to take a lot of time. We can also try reducing the sparse one-hot\n",
        "encoded matrices using singular value decomposition. This is a very common\n",
        "method of extracting topics in natural language processing."
      ],
      "metadata": {
        "id": "g5aW78e7mTPf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Random Forest w/ TruncatedSVD one-hot encoded matrices"
      ],
      "metadata": {
        "id": "-_EpERNum0RL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile ohe_svd_rf.py\n",
        "\n",
        "import pandas as pd\n",
        "from scipy import sparse\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "def run(fold):\n",
        "  # Load the full training data with folds\n",
        "  df = pd.read_csv('../input/cat_train_folds.csv')\n",
        "\n",
        "  # all columns are features except id, target and kfold columns\n",
        "  features = [\n",
        "  f for f in df.columns if f not in (\"id\", \"target\", \"kfold\")\n",
        "  ]\n",
        "  \n",
        "  # fill all NaN values with NONE\n",
        "  # note that I am converting all columns to \"strings\"\n",
        "  # it doesn’t matter because all are categories\n",
        "  for col in features:\n",
        "    df.loc[:, col] = df[col].astype(str).fillna(\"NONE\")\n",
        "\n",
        "  # get training data using folds\n",
        "  df_train = df[df.kfold != fold].reset_index(drop=True)\n",
        "\n",
        "  # get validation data using folds\n",
        "  df_valid = df[df.kfold == fold].reset_index(drop=True)\n",
        "\n",
        "  # initialize OneHotEncoder from scikit-learn\n",
        "  ohe = OneHotEncoder()\n",
        "  \n",
        "  # fit ohe on training + validation features\n",
        "  full_data = pd.concat(\n",
        "  [df_train[features], df_valid[features]],\n",
        "  axis=0\n",
        "  )\n",
        "\n",
        "  ohe.fit(full_data[features])\n",
        "  \n",
        "  # transform training data\n",
        "  x_train = ohe.transform(df_train[features])\n",
        "  \n",
        "  # transform validation data\n",
        "  x_valid = ohe.transform(df_valid[features])\n",
        "  \n",
        "  # initialize TruncatedSVD\n",
        "  svd = TruncatedSVD(n_components=120)\n",
        "\n",
        "  # fit svd on full sparse training data\n",
        "  full_sparse = sparse.vstack((x_train, x_valid))\n",
        "  svd.fit(full_sparse)\n",
        "\n",
        "  # transform sparse training data\n",
        "  x_train = svd.transform(x_train)\n",
        "\n",
        "  # transform sparse validation data\n",
        "  x_valid = svd.transform(x_valid)\n",
        "\n",
        "  # initialize Random Forest model\n",
        "  model = RandomForestClassifier(n_jobs=-1)\n",
        "\n",
        "  # fit model on training data (ohe)\n",
        "  model.fit(x_train, df_train.target.values)\n",
        "\n",
        "  # predict on validation data\n",
        "  # we need the probability values as we are calculating AUC\n",
        "  # we will use the probability of 1s\n",
        "  valid_preds = model.predict_proba(x_valid)[:, 1]\n",
        "\n",
        "  # get roc auc score\n",
        "  auc = roc_auc_score(df_valid.target.values, valid_preds)\n",
        "\n",
        "  # print auc\n",
        "  print(f\"Fold = {fold}, AUC = {auc}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  for fold_ in range(5):\n",
        "    run(fold_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TMgQlliZmlzD",
        "outputId": "f978b0ec-790d-4977-8c2d-a40e6941b24f"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing ohe_svd_rf.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python ohe_svd_rf.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "roxQV9xqnnyf",
        "outputId": "86935908-3024-44d1-cc88-e8494f3cd258"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\", line 935, in retrieve\n",
            "    self._output.extend(job.get(timeout=self.timeout))\n",
            "  File \"/usr/lib/python3.7/multiprocessing/pool.py\", line 651, in get\n",
            "    self.wait(timeout)\n",
            "  File \"/usr/lib/python3.7/multiprocessing/pool.py\", line 648, in wait\n",
            "    self._event.wait(timeout)\n",
            "  File \"/usr/lib/python3.7/threading.py\", line 552, in wait\n",
            "    signaled = self._cond.wait(timeout)\n",
            "  File \"/usr/lib/python3.7/threading.py\", line 296, in wait\n",
            "    waiter.acquire()\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"ohe_svd_rf.py\", line 80, in <module>\n",
            "    run(fold_)\n",
            "  File \"ohe_svd_rf.py\", line 65, in run\n",
            "    model.fit(x_train, df_train.target.values)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/_forest.py\", line 467, in fit\n",
            "    for i, t in enumerate(trees)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\", line 1056, in __call__\n",
            "    self.retrieve()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\", line 957, in retrieve\n",
            "    backend.abort_everything(ensure_ready=ensure_ready)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/joblib/_parallel_backends.py\", line 257, in abort_everything\n",
            "    self.terminate()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/joblib/_parallel_backends.py\", line 243, in terminate\n",
            "    self._pool.terminate()  # terminate does a join()\n",
            "  File \"/usr/lib/python3.7/multiprocessing/pool.py\", line 548, in terminate\n",
            "    self._terminate()\n",
            "  File \"/usr/lib/python3.7/multiprocessing/util.py\", line 224, in __call__\n",
            "    res = self._callback(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.7/multiprocessing/pool.py\", line 594, in _terminate_pool\n",
            "    worker_handler.join()\n",
            "  File \"/usr/lib/python3.7/threading.py\", line 1044, in join\n",
            "    self._wait_for_tstate_lock()\n",
            "  File \"/usr/lib/python3.7/threading.py\", line 1060, in _wait_for_tstate_lock\n",
            "    elif lock.acquire(block, timeout):\n",
            "KeyboardInterrupt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We see that it is even worse. It seems like the best method for this problem is onehot\n",
        "encoding with logistic regression. Random forest appears to be taking way too\n",
        "much time. Maybe we can give XGBoost a try. Since it’s a\n",
        "tree-based algorithm, we will use label encoded data."
      ],
      "metadata": {
        "id": "cH5TFhHantnE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## XGBoost with LabelEncoder"
      ],
      "metadata": {
        "id": "eY5_kab-oyBK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile lbl_xgb.py\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "import xgboost as xgb\n",
        "\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "def run(fold):\n",
        "  # Load the full training data with folds\n",
        "  df = pd.read_csv('../input/cat_train_folds.csv')\n",
        "\n",
        "  # all columns are features except id, target and kfold columns\n",
        "  features = [\n",
        "  f for f in df.columns if f not in (\"id\", \"target\", \"kfold\")\n",
        "  ]\n",
        "  \n",
        "  # fill all NaN values with NONE\n",
        "  # note that I am converting all columns to \"strings\"\n",
        "  # it doesn’t matter because all are categories\n",
        "  for col in features:\n",
        "    df.loc[:, col] = df[col].astype(str).fillna(\"NONE\")\n",
        "\n",
        "  # label encode the features\n",
        "  for col in features:\n",
        "    lbl = LabelEncoder()\n",
        "\n",
        "    # fit label encoder on all data\n",
        "    lbl.fit(df[col])\n",
        "\n",
        "    # transform all the data\n",
        "    df.loc[:, col] = lbl.transform(df[col])\n",
        "\n",
        "  # get training data using folds\n",
        "  df_train = df[df.kfold != fold].reset_index(drop=True)\n",
        "\n",
        "  # get validation data using folds\n",
        "  df_valid = df[df.kfold == fold].reset_index(drop=True)\n",
        "  \n",
        "  # get training data\n",
        "  x_train = df_train[features].values\n",
        "  \n",
        "  # get validation data\n",
        "  x_valid = df_valid[features].values\n",
        "  \n",
        "  # initialize XGBoost model\n",
        "  model = XGBClassifier(n_jobs=-1,\n",
        "                        max_depth=7,\n",
        "                        n_estimators=200)\n",
        "\n",
        "  # fit model on training data (ohe)\n",
        "  model.fit(x_train, df_train.target.values)\n",
        "\n",
        "  # predict on validation data\n",
        "  # we need the probability values as we are calculating AUC\n",
        "  # we will use the probability of 1s\n",
        "  valid_preds = model.predict_proba(x_valid)[:, 1]\n",
        "\n",
        "  # get roc auc score\n",
        "  auc = roc_auc_score(df_valid.target.values, valid_preds)\n",
        "\n",
        "  # print auc\n",
        "  print(f\"Fold = {fold}, AUC = {auc}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  for fold_ in range(5):\n",
        "    run(fold_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nWavCqgTnyHK",
        "outputId": "03f721a8-857e-4582-ac97-d18249e4460d"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting lbl_rf.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# US Adult Census data\n",
        "Let’s change the dataset to another\n",
        "dataset with a lot of categorical variables. One more famous dataset is US adult\n",
        "census data. The dataset contains some features, and your job is to predict the\n",
        "salary bracket."
      ],
      "metadata": {
        "id": "Sp9l2JOSoMY8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"../data/adult.csv\")\n",
        "\n",
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 330
        },
        "id": "LHTz9kmsoWWr",
        "outputId": "f1276635-2ff8-4ff2-c87c-72170bffa33a"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   age workclass  fnlwgt     education  education.num marital.status  \\\n",
              "0   90         ?   77053       HS-grad              9        Widowed   \n",
              "1   82   Private  132870       HS-grad              9        Widowed   \n",
              "2   66         ?  186061  Some-college             10        Widowed   \n",
              "3   54   Private  140359       7th-8th              4       Divorced   \n",
              "4   41   Private  264663  Some-college             10      Separated   \n",
              "\n",
              "          occupation   relationship   race     sex  capital.gain  \\\n",
              "0                  ?  Not-in-family  White  Female             0   \n",
              "1    Exec-managerial  Not-in-family  White  Female             0   \n",
              "2                  ?      Unmarried  Black  Female             0   \n",
              "3  Machine-op-inspct      Unmarried  White  Female             0   \n",
              "4     Prof-specialty      Own-child  White  Female             0   \n",
              "\n",
              "   capital.loss  hours.per.week native.country income  \n",
              "0          4356              40  United-States  <=50K  \n",
              "1          4356              18  United-States  <=50K  \n",
              "2          4356              40  United-States  <=50K  \n",
              "3          3900              40  United-States  <=50K  \n",
              "4          3900              40  United-States  <=50K  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-8313921e-007f-47f3-9b0e-463a8ac84fe5\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>age</th>\n",
              "      <th>workclass</th>\n",
              "      <th>fnlwgt</th>\n",
              "      <th>education</th>\n",
              "      <th>education.num</th>\n",
              "      <th>marital.status</th>\n",
              "      <th>occupation</th>\n",
              "      <th>relationship</th>\n",
              "      <th>race</th>\n",
              "      <th>sex</th>\n",
              "      <th>capital.gain</th>\n",
              "      <th>capital.loss</th>\n",
              "      <th>hours.per.week</th>\n",
              "      <th>native.country</th>\n",
              "      <th>income</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>90</td>\n",
              "      <td>?</td>\n",
              "      <td>77053</td>\n",
              "      <td>HS-grad</td>\n",
              "      <td>9</td>\n",
              "      <td>Widowed</td>\n",
              "      <td>?</td>\n",
              "      <td>Not-in-family</td>\n",
              "      <td>White</td>\n",
              "      <td>Female</td>\n",
              "      <td>0</td>\n",
              "      <td>4356</td>\n",
              "      <td>40</td>\n",
              "      <td>United-States</td>\n",
              "      <td>&lt;=50K</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>82</td>\n",
              "      <td>Private</td>\n",
              "      <td>132870</td>\n",
              "      <td>HS-grad</td>\n",
              "      <td>9</td>\n",
              "      <td>Widowed</td>\n",
              "      <td>Exec-managerial</td>\n",
              "      <td>Not-in-family</td>\n",
              "      <td>White</td>\n",
              "      <td>Female</td>\n",
              "      <td>0</td>\n",
              "      <td>4356</td>\n",
              "      <td>18</td>\n",
              "      <td>United-States</td>\n",
              "      <td>&lt;=50K</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>66</td>\n",
              "      <td>?</td>\n",
              "      <td>186061</td>\n",
              "      <td>Some-college</td>\n",
              "      <td>10</td>\n",
              "      <td>Widowed</td>\n",
              "      <td>?</td>\n",
              "      <td>Unmarried</td>\n",
              "      <td>Black</td>\n",
              "      <td>Female</td>\n",
              "      <td>0</td>\n",
              "      <td>4356</td>\n",
              "      <td>40</td>\n",
              "      <td>United-States</td>\n",
              "      <td>&lt;=50K</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>54</td>\n",
              "      <td>Private</td>\n",
              "      <td>140359</td>\n",
              "      <td>7th-8th</td>\n",
              "      <td>4</td>\n",
              "      <td>Divorced</td>\n",
              "      <td>Machine-op-inspct</td>\n",
              "      <td>Unmarried</td>\n",
              "      <td>White</td>\n",
              "      <td>Female</td>\n",
              "      <td>0</td>\n",
              "      <td>3900</td>\n",
              "      <td>40</td>\n",
              "      <td>United-States</td>\n",
              "      <td>&lt;=50K</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>41</td>\n",
              "      <td>Private</td>\n",
              "      <td>264663</td>\n",
              "      <td>Some-college</td>\n",
              "      <td>10</td>\n",
              "      <td>Separated</td>\n",
              "      <td>Prof-specialty</td>\n",
              "      <td>Own-child</td>\n",
              "      <td>White</td>\n",
              "      <td>Female</td>\n",
              "      <td>0</td>\n",
              "      <td>3900</td>\n",
              "      <td>40</td>\n",
              "      <td>United-States</td>\n",
              "      <td>&lt;=50K</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8313921e-007f-47f3-9b0e-463a8ac84fe5')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-8313921e-007f-47f3-9b0e-463a8ac84fe5 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-8313921e-007f-47f3-9b0e-463a8ac84fe5');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.income.value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6-LiOkeJoovB",
        "outputId": "c5d09f74-040e-4f28-dae7-e9f46aae3b78"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<=50K    24720\n",
              ">50K      7841\n",
              "Name: income, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We see that there are 7841 instances with income greater than 50K USD. This is\n",
        "~24% of the total number of samples. Thus, we will keep the evaluation same as\n",
        "the cat-in-the-dat dataset, i.e. AUC. Before we start modelling, for simplicity, we\n",
        "will be dropping a few columns, which are numerical, namely:\n",
        "- fnlwgt\n",
        "- age\n",
        "- capital.gain\n",
        "- capital.loss\n",
        "- hours.per.week"
      ],
      "metadata": {
        "id": "qiFht2b8qP1w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Logistic Regression\n"
      ],
      "metadata": {
        "id": "ZJJs3vtgqZM2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile ohe_logres_adult.py\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "def run(fold):\n",
        "  # Load the full training data with folds\n",
        "  df = pd.read_csv('../input/adult_folds.csv')\n",
        "\n",
        "  # list of numerical columns\n",
        "  num_cols = [\n",
        "  \"fnlwgt\",\n",
        "  \"age\",\n",
        "  \"capital.gain\",\n",
        "  \"capital.loss\",\n",
        "  \"hours.per.week\"\n",
        "  ]\n",
        "\n",
        "  # drop numerical columns\n",
        "  df = df.drop(num_cols, axis=1)\n",
        "\n",
        "  # map targets to 0s and 1s\n",
        "  target_mapping = {\n",
        "  \"<=50K\": 0,\n",
        "  \">50K\": 1\n",
        "  }\n",
        "  df.loc[:, \"income\"] = df.income.map(target_mapping)\n",
        "\n",
        "  # all columns are features except id, target and kfold columns\n",
        "  features = [\n",
        "  f for f in df.columns if f not in (\"kfold\", \"income\")\n",
        "  ]\n",
        "  # fill all NaN values with NONE\n",
        "  # note that I am converting all columns to \"strings\"\n",
        "  # it doesn’t matter because all are categories\n",
        "\n",
        "  for col in features:\n",
        "    df.loc[:, col] = df[col].astype(str).fillna(\"NONE\")\n",
        "\n",
        "  # get training data using folds\n",
        "  df_train = df[df.kfold != fold].reset_index(drop=True)\n",
        "\n",
        "  # get validation data using folds\n",
        "  df_valid = df[df.kfold == fold].reset_index(drop=True)\n",
        "\n",
        "  # initialize OneHotEncoder from scikit-learn\n",
        "  ohe = OneHotEncoder()\n",
        "  # fit ohe on training + validation features\n",
        "  full_data = pd.concat(\n",
        "  [df_train[features], df_valid[features]],\n",
        "  axis=0\n",
        "  )\n",
        "\n",
        "  ohe.fit(full_data[features])\n",
        "  \n",
        "  # transform training data\n",
        "  x_train = ohe.transform(df_train[features])\n",
        "  \n",
        "  # transform validation data\n",
        "  x_valid = ohe.transform(df_valid[features])\n",
        "  \n",
        "  # initialize Logistic Regression model\n",
        "  model = LogisticRegression()\n",
        "\n",
        "  # fit model on training data (ohe)\n",
        "  model.fit(x_train, df_train.income.values)\n",
        "\n",
        "  # predict on validation data\n",
        "  # we need the probability values as we are calculating AUC\n",
        "  # we will use the probability of 1s\n",
        "  valid_preds = model.predict_proba(x_valid)[:, 1]\n",
        "\n",
        "  # get roc auc score\n",
        "  auc = roc_auc_score(df_valid.income.values, valid_preds)\n",
        "\n",
        "  # print auc\n",
        "  print(f\"Fold = {fold}, AUC = {auc}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  for fold_ in range(5):\n",
        "    run(fold_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ul2LDVgzqbWj",
        "outputId": "f37d7af6-c006-42b3-b79f-c64f4160d21f"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing ohe_logres_adult.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -W ignore ohe_logres_adult.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "25UcBi06rABL",
        "outputId": "a86a0f91-9150-4ec7-9622-daeef2b9d2d0"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold = 0, AUC = 0.8794835490830637\n",
            "Fold = 1, AUC = 0.887601339079321\n",
            "Fold = 2, AUC = 0.8852609687685753\n",
            "Fold = 3, AUC = 0.8681271052110165\n",
            "Fold = 4, AUC = 0.8728581541840037\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## XGBoost with numerical columns"
      ],
      "metadata": {
        "id": "2T7nhtItrCic"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile lbl_xgb_adult.py\n",
        "import pandas as pd\n",
        "\n",
        "import xgboost as xgb\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "def run(fold):\n",
        "  # Load the full training data with folds\n",
        "  df = pd.read_csv('../input/adult_folds.csv')\n",
        "\n",
        "  # list of numerical columns\n",
        "  num_cols = [\n",
        "  \"fnlwgt\",\n",
        "  \"age\",\n",
        "  \"capital.gain\",\n",
        "  \"capital.loss\",\n",
        "  \"hours.per.week\"\n",
        "  ]\n",
        "\n",
        "  # drop numerical columns\n",
        "  df = df.drop(num_cols, axis=1)\n",
        "\n",
        "  # map targets to 0s and 1s\n",
        "  target_mapping = {\n",
        "  \"<=50K\": 0,\n",
        "  \">50K\": 1\n",
        "  }\n",
        "  df.loc[:, \"income\"] = df.income.map(target_mapping)\n",
        "\n",
        "  # all columns are features except id, target and kfold columns\n",
        "  features = [\n",
        "  f for f in df.columns if f not in (\"kfold\", \"income\")\n",
        "  ]\n",
        "  # fill all NaN values with NONE\n",
        "  # note that I am converting all columns to \"strings\"\n",
        "  # it doesn’t matter because all are categories\n",
        "\n",
        "  for col in features:\n",
        "    df.loc[:, col] = df[col].astype(str).fillna(\"NONE\")\n",
        "\n",
        "  # now its time to label encode the features\n",
        "  for col in features:\n",
        "    if col not in num_cols:\n",
        "      # initialize LabelEncoder for each feature column\n",
        "      lbl = LabelEncoder()\n",
        "      # fit label encoder on all data\n",
        "      lbl.fit(df[col])\n",
        "      # transform all the data\n",
        "      df.loc[:, col] = lbl.transform(df[col])\n",
        "\n",
        "  # get training data using folds\n",
        "  df_train = df[df.kfold != fold].reset_index(drop=True)\n",
        "\n",
        "  # get validation data using folds\n",
        "  df_valid = df[df.kfold == fold].reset_index(drop=True)\n",
        "\n",
        "  x_train = df_train[features].values\n",
        " \n",
        "  # get validation data\n",
        "  x_valid = df_valid[features].values\n",
        " \n",
        "  # initialize xgboost model\n",
        "  model = xgb.XGBClassifier(\n",
        "  n_jobs=-1\n",
        "  )\n",
        "\n",
        "  # fit model on training data (ohe)\n",
        "  model.fit(x_train, df_train.income.values)\n",
        "\n",
        "  # predict on validation data\n",
        "  # we need the probability values as we are calculating AUC\n",
        "  # we will use the probability of 1s\n",
        "  valid_preds = model.predict_proba(x_valid)[:, 1]\n",
        "\n",
        "  # get roc auc score\n",
        "  auc = roc_auc_score(df_valid.income.values, valid_preds)\n",
        "\n",
        "  # print auc\n",
        "  print(f\"Fold = {fold}, AUC = {auc}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  for fold_ in range(5):\n",
        "    run(fold_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "riEpMT31rP-r",
        "outputId": "1d0bb6a3-be22-4c61-f511-22f0e3d36912"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing lbl_xgb_adult.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python lbl_xgb_adult.py"
      ],
      "metadata": {
        "id": "ItTBUCtSrvRB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## XGBoost with feature engineering"
      ],
      "metadata": {
        "id": "YjqLw3nLryKE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile xgb_num_feat.py\n",
        "\n",
        "import itertools\n",
        "import pandas as pd\n",
        "import xgboost as xgb\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn import preprocessing\n",
        "\n",
        "def feature_engineering(df, cat_cols):\n",
        "  \"\"\"\n",
        "  This function is used for feature engineering\n",
        "  :param df: the pandas dataframe with train/test data\n",
        "  :param cat_cols: list of categorical columns\n",
        "  :return: dataframe with new features\n",
        "  \"\"\"\n",
        "  # this will create all 2-combinations of values\n",
        "  # in this list\n",
        "  # for example:\n",
        "  # list(itertools.combinations([1,2,3], 2)) will return\n",
        "  # [(1, 2), (1, 3), (2, 3)]\n",
        "  combi = list(itertools.combinations(cat_cols, 2))\n",
        "  for c1, c2 in combi:\n",
        "    df.loc[\n",
        "    :,\n",
        "    c1 + \"_\" + c2\n",
        "    ] = df[c1].astype(str) + \"_\" + df[c2].astype(str)\n",
        "  return df\n",
        "\n",
        "def run(fold):\n",
        "  # load the full training data with folds\n",
        "  df = pd.read_csv('../input/adult_folds.csv')\n",
        "\n",
        "  # list of numerical columns\n",
        "  num_cols = [\n",
        "  \"fnlwgt\",\n",
        "  \"age\",\n",
        "  \"capital.gain\",\n",
        "  \"capital.loss\",\n",
        "  \"hours.per.week\"\n",
        "  ]\n",
        "\n",
        "  # drop numerical columns\n",
        "  df = df.drop(num_cols, axis=1)\n",
        "\n",
        "  # map targets to 0s and 1s\n",
        "  target_mapping = {\n",
        "  \"<=50K\": 0,\n",
        "  \">50K\": 1\n",
        "  }\n",
        "  df.loc[:, \"income\"] = df.income.map(target_mapping)\n",
        "\n",
        "  # list of categorical columns for feature engineering\n",
        "  cat_cols = [\n",
        "  c for c in df.columns if c not in num_cols\n",
        "  and c not in (\"kfold\", \"income\")\n",
        "  ]\n",
        "  \n",
        "  # add new features\n",
        "  df = feature_engineering(df, cat_cols)\n",
        "  \n",
        "  # all columns are features except id, target and kfold columns\n",
        "  features = [\n",
        "  f for f in df.columns if f not in (\"kfold\", \"income\")\n",
        "  ]\n",
        "  # fill all NaN values with NONE\n",
        "  # note that I am converting all columns to \"strings\"\n",
        "  # it doesn’t matter because all are categories\n",
        "\n",
        "  for col in features:\n",
        "    df.loc[:, col] = df[col].astype(str).fillna(\"NONE\")\n",
        "\n",
        "  # now its time to label encode the features\n",
        "  for col in features:\n",
        "    if col not in num_cols:\n",
        "      # initialize LabelEncoder for each feature column\n",
        "      lbl = LabelEncoder()\n",
        "      # fit label encoder on all data\n",
        "      lbl.fit(df[col])\n",
        "      # transform all the data\n",
        "      df.loc[:, col] = lbl.transform(df[col])\n",
        "\n",
        "  # get training data using folds\n",
        "  df_train = df[df.kfold != fold].reset_index(drop=True)\n",
        "\n",
        "  # get validation data using folds\n",
        "  df_valid = df[df.kfold == fold].reset_index(drop=True)\n",
        "\n",
        "  x_train = df_train[features].values\n",
        " \n",
        "  # get validation data\n",
        "  x_valid = df_valid[features].values\n",
        " \n",
        "  # initialize xgboost model\n",
        "  model = xgb.XGBClassifier(\n",
        "  n_jobs=-1\n",
        "  )\n",
        "\n",
        "  # fit model on training data (ohe)\n",
        "  model.fit(x_train, df_train.income.values)\n",
        "\n",
        "  # predict on validation data\n",
        "  # we need the probability values as we are calculating AUC\n",
        "  # we will use the probability of 1s\n",
        "  valid_preds = model.predict_proba(x_valid)[:, 1]\n",
        "\n",
        "  # get roc auc score\n",
        "  auc = roc_auc_score(df_valid.income.values, valid_preds)\n",
        "\n",
        "  # print auc\n",
        "  print(f\"Fold = {fold}, AUC = {auc}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  for fold_ in range(5):\n",
        "    run(fold_)  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rk9Y8a3Xrz3K",
        "outputId": "f7f075f8-3901-4a7e-a591-c776d891c8af"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing xgb_num_feat.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is a very naïve way of creating features from categorical columns. One should\n",
        "take a look at the data and see which combinations make the most sense. If you use\n",
        "this method, you might end up creating a lot of features, and in that case, you will need to use some kind of feature selection to select the best features"
      ],
      "metadata": {
        "id": "aNcori8hsTMK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python xgb_num_feat.py"
      ],
      "metadata": {
        "id": "DG9Yk8AYsPwN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "One more way of feature engineering from categorical features is to use target\n",
        "encoding. However, you have to be very careful here as this might overfit your\n",
        "model. Target encoding is a technique in which you map each category in a given\n",
        "feature to its mean target value, but this must always be done in a cross-validated\n",
        "manner. It means that the first thing you do is create the folds, and then use those\n",
        "folds to create target encoding features for different columns of the data in the same\n",
        "way you fit and predict the model on folds. So, if you have created 5 folds, you\n",
        "have to create target encoding 5 times such that in the end, you have encoding for\n",
        "variables in each fold which are not derived from the same fold. And then when\n",
        "you fit your model, you must use the same folds again. Target encoding for unseen\n",
        "test data can be derived from the full training data or can be an average of all the 5\n",
        "folds."
      ],
      "metadata": {
        "id": "H5MHRc77sa4i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile target_encoding.py\n",
        "import copy\n",
        "import pandas as pd\n",
        "from sklearn import metrics\n",
        "from sklearn import preprocessing\n",
        "import xgboost as xgb\n",
        "def mean_target_encoding(data):\n",
        "  # make a copy of dataframe\n",
        "  df = copy.deepcopy(data)\n",
        "  \n",
        "  # list of numerical columns\n",
        "  num_cols = [\n",
        "  \"fnlwgt\",\n",
        "  \"age\",\n",
        "  \"capital.gain\",\n",
        "  \"capital.loss\",\n",
        "  \"hours.per.week\"\n",
        "  ]\n",
        "  \n",
        "  # map targets to 0s and 1s\n",
        "  target_mapping = {\n",
        "  \"<=50K\": 0,\n",
        "  \">50K\": 1\n",
        "  }\n",
        "  df.loc[:, \"income\"] = df.income.map(target_mapping)\n",
        "  \n",
        "  # all columns are features except income and kfold columns\n",
        "  features = [\n",
        "  f for f in df.columns if f not in (\"kfold\", \"income\")\n",
        "  and f not in num_cols\n",
        "  ]\n",
        "  \n",
        "  # fill all NaN values with NONE\n",
        "  # note that I am converting all columns to \"strings\"\n",
        "  # it doesnt matter because all are categories\n",
        "  for col in features:\n",
        "  \n",
        "  # do not encode the numerical columns\n",
        "  if col not in num_cols:\n",
        "    df.loc[:, col] = df[col].astype(str).fillna(\"NONE\")\n",
        "# now its time to label encode the features\n",
        "  for col in features:\n",
        "    if col not in num_cols:\n",
        "      # initialize LabelEncoder for each feature column\n",
        "      lbl = preprocessing.LabelEncoder()\n",
        "      # fit label encoder on all data\n",
        "      lbl.fit(df[col])\n",
        "      # transform all the data\n",
        "      df.loc[:, col] = lbl.transform(df[col])\n",
        "  \n",
        "  # a list to store 5 validation dataframes\n",
        "  encoded_dfs = []\n",
        "  \n",
        "  # go over all folds\n",
        "  for fold in range(5):\n",
        "  \n",
        "    # fetch training and validation data\n",
        "    df_train = df[df.kfold != fold].reset_index(drop=True)\n",
        "    df_valid = df[df.kfold == fold].reset_index(drop=True)\n",
        "  \n",
        "    # for all feature columns, i.e. categorical columns\n",
        "    for column in features:\n",
        "  \n",
        "      # create dict of category:mean target\n",
        "      mapping_dict = dict(\n",
        "      df_train.groupby(column)[\"income\"].mean()\n",
        "      )\n",
        "      # column_enc is the new column we have with mean encoding\n",
        "      df_valid.loc[\n",
        "      :, column + \"_enc\"\n",
        "      ] = df_valid[column].map(mapping_dict)\n",
        "    # append to our list of encoded validation dataframes\n",
        "    encoded_dfs.append(df_valid)\n",
        "  # create full data frame again and return\n",
        "  encoded_df = pd.concat(encoded_dfs, axis=0)\n",
        "  return encoded_df\n",
        "\n",
        "def run(df, fold):\n",
        "  # note that folds are same as before\n",
        "  \n",
        "  # get training data using folds\n",
        "  df_train = df[df.kfold != fold].reset_index(drop=True)\n",
        "  \n",
        "  # get validation data using folds\n",
        "  df_valid = df[df.kfold == fold].reset_index(drop=True)\n",
        "\n",
        "  # all columns are features except income and kfold columns\n",
        "  features = [\n",
        "  f for f in df.columns if f not in (\"kfold\", \"income\")\n",
        "  ]\n",
        "  \n",
        "  # scale training data\n",
        "  x_train = df_train[features].values\n",
        "  \n",
        "  # scale validation data\n",
        "  x_valid = df_valid[features].values\n",
        "  \n",
        "  # initialize xgboost model\n",
        "  model = xgb.XGBClassifier(\n",
        "  n_jobs=-1,\n",
        "  max_depth=7\n",
        "  )\n",
        "  \n",
        "  # fit model on training data (ohe)\n",
        "  model.fit(x_train, df_train.income.values)\n",
        "  \n",
        "  # predict on validation data\n",
        "  # we need the probability values as we are calculating AUC\n",
        "  # we will use the probability of 1s\n",
        "  valid_preds = model.predict_proba(x_valid)[:, 1]\n",
        "  \n",
        "  # get roc auc score\n",
        "  auc = metrics.roc_auc_score(df_valid.income.values, valid_preds)\n",
        "  \n",
        "  # print auc\n",
        "  print(f\"Fold = {fold}, AUC = {auc}\")\n",
        "  \n",
        "  if __name__ == \"__main__\":\n",
        "  \n",
        "    # read data\n",
        "    df = pd.read_csv(\"../input/adult_folds.csv\")\n",
        "    \n",
        "    # create mean target encoded categories and\n",
        "    # munge data\n",
        "    df = mean_target_encoding(df)\n",
        "    # run training and validation for 5 folds\n",
        "    for fold_ in range(5):\n",
        "      run(df, fold_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qHcHQd6zsjGc",
        "outputId": "f4f7364d-500b-409c-ab92-cd6ffad2fdf8"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing target_encoding.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python target_encoding.py"
      ],
      "metadata": {
        "id": "yAFBhYECtLmo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nice! It seems like we have improved again. However, you must be very careful\n",
        "when using target encoding as it is too prone to overfitting. When we use target\n",
        "encoding, it’s better to use some kind of smoothing or adding noise in the encoded\n",
        "values. Scikit-learn has contrib repository which has target encoding with\n",
        "smoothing, or you can create your own smoothing. Smoothing introduces some\n",
        "kind of regularization that helps with not overfitting the model. It’s not very\n",
        "difficult.\n",
        "\n"
      ],
      "metadata": {
        "id": "15xYtACCtN67"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Neural Network with entity embeddings\n",
        "\n",
        "In entity embeddings, the\n",
        "categories are represented as vectors. We represent categories by vectors in both\n",
        "binarization and one hot encoding approaches. But what if we have tens of\n",
        "thousands of categories. This will create huge matrices and will take a long time for\n",
        "us to train complicated models. We can thus represent them by vectors with float\n",
        "values instead.\n",
        "The idea is super simple. You have an embedding layer for each categorical feature.\n",
        "So, every category in a column can now be mapped to an embedding (like mapping\n",
        "words to embeddings in natural language processing). You then reshape these\n",
        "embeddings to their dimension to make them flat and then concatenate all the\n",
        "flattened inputs embeddings. Then add a bunch of dense layers, an output layer and\n",
        "you are done"
      ],
      "metadata": {
        "id": "1dj6bgtatV9b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile entity_embeddings.py\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "1JuPX0VrtYvy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}